{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","!pip install shap\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import shap\n","import seaborn as sn\n","import sys\n","import sklearn\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from IPython import display\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","\n","sn.set(style='whitegrid')\n","pd.set_option('display.max_columns', None)\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)\n","print(\"shap version:\", shap.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["##import your data\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"_uAaUKFV7USL","metadata":{"id":"_uAaUKFV7USL"},"outputs":[],"source":["# Step 1: Get all columns in the file without loading the full data\n","all_columns = pd.read_csv(file_path, nrows=0).columns.tolist()\n","\n","# Step 2: Define columns you want to exclude\n","columns_to_exclude = ['TRISS_Death', 'TRISS', 'TRISS_b_neg', 'TRISS_b', 'TRISS_AGE', 'RTS', 'RTS_GCS', 'RTS_SBP', 'RTS_RR']\n","\n","# Step 3: Create the list of columns to include\n","columns_to_include = [col for col in all_columns if col not in columns_to_exclude]\n","\n","# Step 4: Load data only with included columns\n","data = pd.read_csv(file_path, usecols=columns_to_include,\n","                   na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'])"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Filter out rows where 'TRAUMATYPE' is 26 (a type of missing), 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass\n","\n","# Create ShockIndex with the required logic\n","data['ShockIndex'] = np.where(\n","    data['SBP'] == 0, 2.0,  # Case where SBP is 0 â†’ set ShockIndex to 2.0\n","    data['PULSERATE'] / data['SBP']  # Normal calculation\n",")\n","\n","# Set ShockIndex to NaN if PULSERATE or SBP is missing\n","data.loc[data['PULSERATE'].isna() | data['SBP'].isna(), 'ShockIndex'] = np.nan\n","\n","##reset indices of the df\n","data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##verify data appears as intended\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing values\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a datafram of all other variables we want to remove from training the model.  Some are available too late, other are essentially duplicates\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'EDDISCHARGEDISPOSITION',\n","                    'HOSPDISCHARGEDISPOSITION',\n","                    'EDDISCHARGEHRS',\n","                    'WITHDRAWALLST',\n","                    'VTEPROPHYLAXISTYPE',\n","                    'TOTALICULOS',\n","                    'TOTALVENTDAYS',\n","                    'VTEPROPHYLAXISHRS',\n","                    'VTEPROPHYLAXISDAYS', 'MORTALITY', 'EDDISCHARGEDAYS','FINALDISCHARGEDAYS','FINALDISCHARGEHRS', 'HMRRHGCTRLSURGDAYS',  'WITHDRAWALLSTHRS',\n","                    'AMERICANINDIAN', 'ASIAN', 'BLACK', 'PACIFICISLANDER', 'RACEOTHER', 'WHITE', 'RACE_NA', 'RACE_UK',\n","                    \"IntracranialVascularInjury\",\n","                    \"BrainStemInjury\",\n","                    \"EDH\",\n","                    \"SAH\",\n","                    \"SDH\",\n","                    \"SkullFx\",\n","                    \"DAI\",\n","                    \"NeckVascularInjury\",\n","                    \"ThoracicVascularInjury\",\n","                    \"AeroDigestiveInjury\",\n","                    \"CardiacInjury\",\n","                    \"LungInjury\",\n","                    \"AbdominalVascular\",\n","                    \"RibFx\",\n","                    \"KidneyInjury\",\n","                    \"StomachInjury\",\n","                    \"SpleenInjury\",\n","                    \"UroGenInternalInjury\",\n","                    \"SCI\",\n","                    \"SpineFx\",\n","                    \"UEAmputation\",\n","                    \"UEVascularInjury\",\n","                    \"UELongBoneFx\",\n","                    \"LEVascularInjury\",\n","                    \"PelvicFx\",\n","                    \"LEAmputation\",\n","                    \"PancreasInjury\",\n","                    \"LELongBoneFx\",\n","                    \"LiverInjury\",\n","                    \"ColorectalInjury\",\n","                    \"SmallBowelInjury\",\n","                    \"NumberOfInjuries\"\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, mortality, and give it its own dataframe\n","\n","Y_data = pd.DataFrame()\n","Y_data['MORTALITY'] = data['MORTALITY']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['MORTALITY'] = Y_data['MORTALITY'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##now drop the these not-used vars from our input space\n","X_data = data.drop(columns=complications_list)\n","X_data.shape"]},{"cell_type":"code","execution_count":null,"id":"72ac3fa7","metadata":{"id":"72ac3fa7"},"outputs":[],"source":["##ensure no missing outcome data\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"]},{"cell_type":"code","source":["##If we have no missing values here, our data is clean\n","Y_clean=Y_data.copy()"],"metadata":{"id":"0Ur-E_ZOenGg"},"id":"0Ur-E_ZOenGg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##if above check passes, outcome data is now clean\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"],"metadata":{"id":"Qw2kkLmrAeIN"},"id":"Qw2kkLmrAeIN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#display columns with less than 50% missing that need to be cleaned\n","\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","to_be_cleaned_column_names"]},{"cell_type":"code","source":["# Display the entire DataFrame without truncation\n","pd.set_option('display.max_columns', None)\n","\n","# Get column names and data types\n","columns_info = []\n","for column_name, dtype in zip(X_data_new.columns, X_data_new.dtypes):\n","    columns_info.append(f\"{column_name}: {dtype}\")\n","\n","formatted_columns_info = \"\\n\".join(columns_info)\n","\n","# Print column names and data types\n","print(\"Column Names and Data Types:\")\n","print(formatted_columns_info)"],"metadata":{"id":"a_7mt7lueXds"},"id":"a_7mt7lueXds","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","\n","##drop any non blunt/penetrating mechanisms\n","try:\n","    X_data_new=X_data_new.drop(['TRAUMATYPE_26', 'TRAUMATYPE_Other/unspecified'], axis=1)\n","except:\n","    pass\n","\n","X_data_new.head()"],"metadata":{"id":"bcHQWaAgeXga"},"id":"bcHQWaAgeXga","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##split into train, test, calibrate sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_data_new, Y_clean, test_size=0.2, random_state=0, stratify=Y_clean)\n","X_train_cal, X_val_cal, Y_train_cal, Y_val_cal = train_test_split(X_train, Y_train, test_size=0.2, random_state=0, stratify=Y_train)"],"metadata":{"id":"oRKxZhxoeM-4"},"id":"oRKxZhxoeM-4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b5ec271e","metadata":{"id":"b5ec271e"},"outputs":[],"source":["##perform median/mode imputation on the inputs vars that are missing\n","for c in to_be_cleaned_column_names:\n","    v = X_train[c]\n","    v_valid = v[~v.isnull()]\n","\n","    if v.dtype == np.dtype('O'):  # Categorical column\n","        mode_value = v_valid.value_counts().index[0]\n","        for df in [X_train, X_test, X_train_cal, X_val_cal]:\n","            df[c] = df[c].fillna(mode_value).astype(object)\n","\n","    else:  # Numeric column\n","        median_value = v_valid.median()\n","        for df in [X_train, X_test, X_train_cal, X_val_cal]:\n","            df[c] = df[c].fillna(median_value)"]},{"cell_type":"code","source":["##now for one-hot encoding\n","\n","# Identify categorical columns from X_train only\n","categorical_column = [c for c in X_train_cal.columns if X_train_cal[c].dtype == np.dtype('O')]\n","\n","# Apply pd.get_dummies to training data\n","X_train_cal = pd.get_dummies(X_train_cal, columns=categorical_column, sparse=False)\n","\n","categorical_column"],"metadata":{"id":"DPn3OJ7aguFf"},"id":"DPn3OJ7aguFf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Align test and validation sets to match training set columns\n","X_test = pd.get_dummies(X_test, columns=categorical_column, sparse=False)\n","X_train = pd.get_dummies(X_train, columns=categorical_column, sparse=False)\n","X_val_cal = pd.get_dummies(X_val_cal, columns=categorical_column, sparse=False)\n","\n","# Ensure same columns across all datasets\n","X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n","X_train = X_train.reindex(columns=X_train.columns, fill_value=0)\n","X_val_cal = X_val_cal.reindex(columns=X_train.columns, fill_value=0)"],"metadata":{"id":"vPAfgjXUguIQ"},"id":"vPAfgjXUguIQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#verify data appears as intended\n","X_train_cal.head()"],"metadata":{"id":"D5xormBHguLH"},"id":"D5xormBHguLH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"31a5eafa","metadata":{"id":"31a5eafa"},"outputs":[],"source":["##verify no missing data in any split dataset\n","print(X_train_cal.isnull().sum().sum())\n","print(X_test.isnull().sum().sum())\n","print(X_val_cal.isnull().sum().sum())"]},{"cell_type":"code","execution_count":null,"id":"mrWz__0NRJtw","metadata":{"id":"mrWz__0NRJtw"},"outputs":[],"source":["##final list of training columns\n","X_train_cal.columns"]},{"cell_type":"code","source":["#verify data is intended size\n","X_test.shape"],"metadata":{"id":"uxxuPQrkv9da"},"id":"uxxuPQrkv9da","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"MkSkefq4RN9D","metadata":{"id":"MkSkefq4RN9D"},"outputs":[],"source":["##store copies of data as tensors\n","X_train_tensor=X_train_cal.copy()\n","Y_train_tensor=Y_train_cal.copy()\n","\n","X_val_tensor=X_val_cal.copy()\n","Y_val_tensor=Y_val_cal.copy()\n","\n","X_test_tensor=X_test.copy()\n","Y_test_tensor=Y_test.copy()"]},{"cell_type":"code","source":["##verify data appears as intended\n","X_test.head()"],"metadata":{"id":"0GKlKCuvKUnw"},"id":"0GKlKCuvKUnw","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"PYoFPWDoRPQW","metadata":{"id":"PYoFPWDoRPQW"},"outputs":[],"source":["##Next step is to normalize data\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train_cal)\n","\n","#normalize the features in the training set\n","X_train_s_cal = scaler.transform(X_train_cal)\n","#normalize the features in the test set\n","print(\"After train/test split, X_test shape:\", X_test.shape)\n","X_test_s = scaler.transform(X_test)\n","print(\"After scaling, X_test_s shape:\", X_test_s.shape)\n","#normalize the features in the val set\n","X_val_s_cal = scaler.transform(X_val_cal)"]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##now, fit model with hyperparameters based on other Jupyternotebook optimization\n","model_best_gb = xgb.XGBClassifier(random_state=0, colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0)\n","model_best_gb.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"L9Wj3__pbnO6","metadata":{"id":"L9Wj3__pbnO6"},"outputs":[],"source":["# Get predicted probabilities for test set (evaluate model)\n","\n","y_prob_gbo_mtp = model_best_gb.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC on test set\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"]},{"cell_type":"code","source":["##now, use Shapley Additive Explanations for better assessment and visualization of feature imprtance\n","\n","your_dataframe = X_train_tensor  # will use this to get column labels, so need the tensor\n","model=model_best_gb\n","\n","# Calculate SHAP values for X_test\n","explainer = shap.TreeExplainer(model)\n","shap_values_test = explainer.shap_values(X_train_s_cal)\n","\n","# Calculate mean absolute SHAP values\n","mean_abs_shap = np.abs(shap_values_test).mean(axis=0)\n","\n","# Sort feature indices based on mean absolute SHAP values\n","sorted_indices = np.argsort(mean_abs_shap)\n","\n","# Identify top 20 most important features\n","top_5_percent_indices = sorted_indices[-20:]\n","\n","# Extract top 20 SHAP values and features\n","top_5_percent_shap_values = shap_values_test[:, top_5_percent_indices]\n","top_5_percent_feature_names = your_dataframe.columns[top_5_percent_indices]\n","\n","# Create horizontal bar chart for top 20 most important features\n","fig1, ax1 = plt.subplots(figsize=(12, 6))\n","bars = ax1.barh(top_5_percent_feature_names, mean_abs_shap[top_5_percent_indices], color='lightblue')\n","ax1.set_xlabel('Mean Absolute SHAP Value')\n","ax1.set_title('Top 5% Most Important Features - Mean Absolute SHAP Values')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"bTR--Qo2XZzh"},"id":"bTR--Qo2XZzh","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"machine_shape":"hm","gpuType":"V6E1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}