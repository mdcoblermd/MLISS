{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","##due to potential module dependencies, we will install DeepTables later\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","import time\n","import os\n","import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import seaborn as sn\n","from google.colab import drive\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, ParameterGrid\n","from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import resample\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","sn.set(style='whitegrid')\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["#import your dataset\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"_uAaUKFV7USL","metadata":{"id":"_uAaUKFV7USL"},"outputs":[],"source":["##specify columns to load from your dataset.  We will only load the columns necessary for the score and the variables necessary for the ISS/TRISS comparisons\n","\n","columns_to_load = ['AGEYEARS', 'TOTALGCS', 'SBP'\n","                  , 'TEMPERATURE'\n","                  , 'PULSERATE', 'TRISS', 'TRISS_Death', 'MORTALITY', 'TRAUMATYPE'\n","                  , 'WEIGHT'\n","                  , 'ISS_05', 'NumberOfInjuries',\n","                   'IntracranialVascularInjury','BrainStemInjury','EDH','SAH','SDH','SkullFx','DAI','NeckVascularInjury','ThoracicVascularInjury','AeroDigestiveInjury',\n","                   'CardiacInjury','LungInjury','AbdominalVascular','RibFx','KidneyInjury','StomachInjury','SpleenInjury','UroGenInternalInjury','SCI','SpineFx',\n","                   'UEAmputation','UEVascularInjury','UELongBoneFx','LEVascularInjury','PelvicFx','LEAmputation','PancreasInjury','LELongBoneFx','LiverInjury',\n","                   'ColorectalInjury','SmallBowelInjury','IPH'\n","                   ]"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Import data and specify missing values\n","data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'], usecols=columns_to_load)\n","\n","\n","# Filter out rows where 'TRAUMATYPE' is 26, 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass\n","\n","##explicitly list variables that need to be present for inclusion and drop cases without these\n","##we cannot compare our score to ISS/TRISS without those metrics, and we need our target outcome mortality\n","required_vars = ['ISS_05', 'TRISS_Death', 'MORTALITY']\n","data = data.dropna(subset=required_vars)\n","\n","# Create ShockIndex with the required logic\n","data['ShockIndex'] = np.where(\n","    data['SBP'] == 0, 2.0,  # Case where SBP is 0 â†’ set ShockIndex to 2.0\n","    data['PULSERATE'] / data['SBP']  # Normal calculation\n",")\n","\n","# Set ShockIndex to NaN if PULSERATE or SBP is missing\n","data.loc[data['PULSERATE'].isna() | data['SBP'].isna(), 'ShockIndex'] = np.nan\n","\n","##reset indices of the df\n","data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##verify data appears as intended\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing values\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a datafram of all complications/vars to remove later.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'MORTALITY', 'TRISS'\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, mortality, and give it its own dataframe\n","\n","Y_data = pd.DataFrame()\n","Y_data['MORTALITY'] = data['MORTALITY']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['MORTALITY'] = Y_data['MORTALITY'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##now drop the outcome from our feature space as well as TRISS, since were using 1-TRISS (aka TRISS_Death) and this varibale is now useless\n","X_data = data.drop(columns=['MORTALITY', 'TRISS'])\n","X_data.shape"]},{"cell_type":"code","execution_count":null,"id":"72ac3fa7","metadata":{"id":"72ac3fa7"},"outputs":[],"source":["##ensure no missing outcome data\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"]},{"cell_type":"code","source":["##If we have no missing values here, our data is clean\n","Y_clean=Y_data.copy()"],"metadata":{"id":"6gVbgUj_k8eC"},"id":"6gVbgUj_k8eC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##if above check passes, outcome data is now clean\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"],"metadata":{"id":"IKUXu3IYAYK3"},"id":"IKUXu3IYAYK3","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","##tried different cutoffs for this (33%, 66%), but 50% yielded best results\n","\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#display columns with less than 50% missing that need to be cleaned\n","\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","to_be_cleaned_column_names"]},{"cell_type":"code","source":["# Rename the 'TRAUMATYPE' column to 'Penetrating' and map the values to 0 and 1\n","X_data_new['Penetrating'] = X_data_new['TRAUMATYPE'].map({'Penetrating': 1, 'Blunt': 0})\n","\n","# Drop the old 'TRAUMATYPE' column\n","X_data_new.drop(columns=['TRAUMATYPE'], inplace=True)\n","\n","print(X_data_new.head())"],"metadata":{"id":"v9lgvYMBeSY_"},"id":"v9lgvYMBeSY_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the entire DataFrame without truncation\n","pd.set_option('display.max_columns', None)\n","\n","# Get column names and data types\n","columns_info = []\n","for column_name, dtype in zip(X_data_new.columns, X_data_new.dtypes):\n","    columns_info.append(f\"{column_name}: {dtype}\")\n","\n","formatted_columns_info = \"\\n\".join(columns_info)\n","\n","# Print column names and data types\n","print(\"Column Names and Data Types:\")\n","print(formatted_columns_info)"],"metadata":{"id":"a_7mt7lueXds"},"id":"a_7mt7lueXds","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","\n","##drop any non blunt/penetrating mechanisms\n","try:\n","    X_data_new=X_data_new.drop(['TRAUMATYPE_26', 'TRAUMATYPE_Other/unspecified'], axis=1)\n","except:\n","    pass\n","\n","X_data_new.head()"],"metadata":{"id":"bcHQWaAgeXga"},"id":"bcHQWaAgeXga","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##split into train, test, calibrate sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_data_new, Y_clean, test_size=0.2, random_state=0, stratify=Y_clean)\n","X_train_cal, X_val_cal, Y_train_cal, Y_val_cal = train_test_split(X_train, Y_train, test_size=0.2, random_state=0, stratify=Y_train)"],"metadata":{"id":"oRKxZhxoeM-4"},"id":"oRKxZhxoeM-4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b5ec271e","metadata":{"id":"b5ec271e"},"outputs":[],"source":["##perform median/mode imputation on the inputs vars that are missing\n","for c in to_be_cleaned_column_names:\n","    v = X_train[c]\n","    v_valid = v[~v.isnull()]\n","\n","    if v.dtype == np.dtype('O'):  # Categorical column\n","        mode_value = v_valid.value_counts().index[0]\n","        for df in [X_train, X_test, X_train_cal, X_val_cal]:\n","            df[c] = df[c].fillna(mode_value).astype(object)\n","\n","    else:  # Numeric column\n","        median_value = v_valid.median()\n","        for df in [X_train, X_test, X_train_cal, X_val_cal]:\n","            df[c] = df[c].fillna(median_value)\n"]},{"cell_type":"code","source":["##now for one-hot encoding\n","\n","# Identify categorical columns from X_train only\n","categorical_column = [c for c in X_train_cal.columns if X_train_cal[c].dtype == np.dtype('O')]\n","\n","# Apply pd.get_dummies to training data\n","X_train_cal = pd.get_dummies(X_train_cal, columns=categorical_column, sparse=False)\n","\n","categorical_column"],"metadata":{"id":"DPn3OJ7aguFf"},"id":"DPn3OJ7aguFf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Align test and validation sets to match training set columns\n","X_test = pd.get_dummies(X_test, columns=categorical_column, sparse=False)\n","X_val_cal = pd.get_dummies(X_val_cal, columns=categorical_column, sparse=False)\n","\n","# Ensure same columns across all datasets\n","X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n","X_val_cal = X_val_cal.reindex(columns=X_train.columns, fill_value=0)"],"metadata":{"id":"vPAfgjXUguIQ"},"id":"vPAfgjXUguIQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#verify data appears as intended\n","X_train_cal.head()"],"metadata":{"id":"D5xormBHguLH"},"id":"D5xormBHguLH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"31a5eafa","metadata":{"id":"31a5eafa"},"outputs":[],"source":["##verify no missing data in any split dataset\n","print(X_train_cal.isnull().sum().sum())\n","print(X_test.isnull().sum().sum())\n","print(X_val_cal.isnull().sum().sum())"]},{"cell_type":"code","execution_count":null,"id":"mrWz__0NRJtw","metadata":{"id":"mrWz__0NRJtw"},"outputs":[],"source":["##final list of training columns\n","X_train_cal.columns"]},{"cell_type":"code","source":["#verify data is intended size\n","X_test.shape"],"metadata":{"id":"uxxuPQrkv9da"},"id":"uxxuPQrkv9da","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now with data cleaned, take comparison vars and move them to their own dataframe prior to dropping\n","new_to_drop = ['TRISS_Death', 'ISS_05']\n","\n","X_ISS=pd.DataFrame()\n","X_ISS['ISS']=X_test['ISS_05']\n","\n","X_TRISS=pd.DataFrame()\n","X_TRISS['TRISS']=X_test['TRISS_Death']"],"metadata":{"id":"o5lm5Yi4qbta"},"id":"o5lm5Yi4qbta","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"MkSkefq4RN9D","metadata":{"id":"MkSkefq4RN9D"},"outputs":[],"source":["##now drop those comparison vars from the data that will be fed to the model\n","X_train_cal.drop(columns=new_to_drop, inplace=True)\n","X_test.drop(columns=new_to_drop, inplace=True)\n","X_val_cal.drop(columns=new_to_drop, inplace=True)\n","\n","\n","##store copies of data as tensors\n","X_train_tensor=X_train_cal.copy()\n","Y_train_tensor=Y_train_cal.copy()\n","\n","X_val_tensor=X_val_cal.copy()\n","Y_val_tensor=Y_val_cal.copy()\n","\n","X_test_tensor=X_test.copy()\n","Y_test_tensor=Y_test.copy()"]},{"cell_type":"code","source":["##verify data appears as intended\n","X_test.head()"],"metadata":{"id":"0GKlKCuvKUnw"},"id":"0GKlKCuvKUnw","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"PYoFPWDoRPQW","metadata":{"id":"PYoFPWDoRPQW"},"outputs":[],"source":["##Next step is to normalize data\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train_cal)\n","\n","#normalize the features in the training set\n","X_train_s_cal = scaler.transform(X_train_cal)\n","#normalize the features in the test set\n","print(\"After train/test split, X_test shape:\", X_test.shape)\n","X_test_s = scaler.transform(X_test)\n","print(\"After scaling, X_test_s shape:\", X_test_s.shape)\n","#normalize the features in the val set\n","X_val_s_cal = scaler.transform(X_val_cal)"]},{"cell_type":"code","execution_count":null,"id":"u3xRL2Wzbi6k","metadata":{"id":"u3xRL2Wzbi6k"},"outputs":[],"source":["##create a dictionary of model hyper-parameter(s)\n","\n","##for KNN\n","n_list=np.arange(1, 803, 2)\n","param_grid_knc = {'n_neighbors':n_list}\n","\n","##for RF\n","param_grid_rf = {\n","    'n_estimators': [100, 200, 400],       ## Number of trees in the forest\n","    'max_depth': [None, 10, 20, 30],       ## Maximum depth of the trees\n","    'min_samples_split': [2, 5, 10],       ## Minimum number of samples required to split an internal node\n","    'min_samples_leaf': [1, 2, 4],         ## Minimum number of samples required to be at a leaf node\n","    'max_features': ['sqrt']               ## Number of features to consider for the best split\n","}\n","\n","##for LR\n","param_grid_lr = {\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],   ## Inverse of regularization strength\n","    'penalty': ['l1', 'l2'],               ## Regularization type\n","    'solver': ['liblinear', 'saga'],       ## Optimization algorithm\n","    'max_iter': [100, 200, 300]            ## Maximum number of iterations\n","    }\n","\n","##this is for XGBoost\n","param_grid_gb = {\n","    'learning_rate': [0.01, 0.05, 0.1],    ## Learning rate\n","    'max_depth': [3, 5, 7],                ## Maximum depth of the trees\n","    'subsample': [0.6, 0.8, 1.0],          ## Subsample ratio of the training instances\n","    'colsample_bytree': [0.6, 0.8, 1.0],   ## Subsample ratio of columns when constructing each tree.\n","    'n_estimators': [100, 150, 200]        ## Number of trees\n","}"]},{"cell_type":"code","source":["##now, optimize GB hyperparameters\n","model_gb=xgb.XGBClassifier(random_state=0) #create an empty model\n","##initialize gridsearch\n","gs_gb = GridSearchCV(estimator=model_gb,\n","                  param_grid=param_grid_gb,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","gs_gb.fit(X_train_s_cal, Y_train_cal)"],"metadata":{"id":"SJOeXi-nt5VT"},"id":"SJOeXi-nt5VT","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##display best parameters\n","gs_gb.best_params_"]},{"cell_type":"code","source":["##display auroc for XGB\n","model_best_gb=gs_gb.best_estimator_\n","y_prob_gbo_mtp = model_best_gb.predict_proba(X_test_s)[:, 1]\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"],"metadata":{"id":"C_Nzdi_tn10p"},"id":"C_Nzdi_tn10p","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"Yin9DcBUVaAi","metadata":{"id":"Yin9DcBUVaAi"},"outputs":[],"source":["##now, optimize LR hyperparameters\n","model_lr=LogisticRegression() #create an empty model\n","##initialize gridsearch\n","gs_lr = GridSearchCV(estimator=model_lr,\n","                  param_grid=param_grid_lr,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","#actually perform hyperparmeter optimization\n","gs_lr.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"L9Wj3__pbnO6","metadata":{"id":"L9Wj3__pbnO6"},"outputs":[],"source":["##display best parameters\n","gs_lr.best_params_"]},{"cell_type":"code","source":["##display auroc for LR\n","model_best_lr=gs_lr.best_estimator_\n","y_prob_lr_mtp = model_best_lr.predict_proba(X_test_s)[:, 1]\n","auroc_lr = roc_auc_score(Y_test, y_prob_lr_mtp)\n","print(f\"AUROC on the test set: {auroc_lr}\")"],"metadata":{"id":"9_fjfWSgn_gu"},"id":"9_fjfWSgn_gu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ##now, optimize RF hyperparameters\n","model_rf=RandomForestClassifier(random_state=0) #create an empty model\n","##initialize gridsearch\n","gs_rf = GridSearchCV(estimator=model_rf,\n","                  param_grid=param_grid_rf,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","#actually perform hyperparmeter optimization\n","gs_rf.fit(X_train_s_cal, Y_train_cal)"],"metadata":{"id":"qsxWzVgSnNIK"},"id":"qsxWzVgSnNIK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##display auroc for RF\n","model_best_rf=gs_rf.best_estimator_\n","y_prob_rf_mtp = model_best_rf.predict_proba(X_test_s)[:, 1]\n","auroc_rf = roc_auc_score(Y_test, y_prob_rf_mtp)\n","print(f\"AUROC on the test set: {auroc_rf}\")"],"metadata":{"id":"mtaAyaW_nOOr"},"id":"mtaAyaW_nOOr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##KNN with GS_CV to optimize hyperparameter\n","from sklearn.neighbors import KNeighborsClassifier\n","model_knno=KNeighborsClassifier() #create an empty model\n","##initialize gridsearch\n","gs_knno = GridSearchCV(estimator=model_knno,\n","                  param_grid=param_grid_knc,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","#actually perform hyperparmeter optimization\n","gs_knno.fit(X_train_s_cal, Y_train_cal)"],"metadata":{"id":"juRk-ojpnPoX"},"id":"juRk-ojpnPoX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##display best parameters\n","gs_knno.best_params_"],"metadata":{"id":"wdqNEkzGnQ_R"},"id":"wdqNEkzGnQ_R","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##display auroc for KNN\n","model_best_knno=gs_knno.best_estimator_\n","y_prob_knno_mtp = model_best_knno.predict_proba(X_test_s)[:, 1]\n","auroc_knno = roc_auc_score(Y_test, y_prob_knno_mtp)\n","print(f\"AUROC on the test set: {auroc_knno}\")"],"metadata":{"id":"DwxA2TzTp65Y"},"id":"DwxA2TzTp65Y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##copy existing dataframes to use in neural networks\n","\n","X_clean_nn_test=X_test_s.copy()\n","Y_clean_nn_test=Y_test.copy()\n","\n","X_clean_nn_train=X_train_s_cal.copy()\n","Y_clean_nn_train=Y_train_cal.copy()\n","\n","X_clean_nn_cal=X_val_s_cal.copy()\n","Y_clean_nn_cal=Y_val_cal.copy()"],"metadata":{"id":"gAXQC6dJnSZN"},"id":"gAXQC6dJnSZN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##ensure data is in pandas dataframe\n","X_train_df = pd.DataFrame(X_clean_nn_train)\n","Y_train_s = pd.Series(Y_clean_nn_train.squeeze())\n","\n","X_val_df = pd.DataFrame(X_clean_nn_cal)\n","Y_val_s = pd.Series(Y_clean_nn_cal.squeeze())\n","\n","X_test_df = pd.DataFrame(X_clean_nn_test)\n","Y_test_s = pd.Series(Y_clean_nn_test.squeeze())"],"metadata":{"id":"lA_sIWOEnTZ5"},"id":"lA_sIWOEnTZ5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deeptables\n","##revert to sklearn 1.5 to resolve dependency issues\n","!pip install scikit-learn==1.5\n","import deeptables\n","print(\"dt version:\", deeptables.__version__)\n","from deeptables.models.deeptable import DeepTable, ModelConfig\n","from deeptables.models.deepnets import DeepFM, WideDeep, DCN"],"metadata":{"id":"Y-y8IKGAnVb9"},"id":"Y-y8IKGAnVb9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit--try DeepFM first\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=DeepFM,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"HJGKjyuinXun"},"id":"HJGKjyuinXun","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"-cTFWksNnav4"},"id":"-cTFWksNnav4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit--try WideDeep second\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=WideDeep,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"PoBOAsVEncQg"},"id":"PoBOAsVEncQg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"8uDs3widndqo"},"id":"8uDs3widndqo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit--try DCN last\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=DCN,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"obp9WqELneH0"},"id":"obp9WqELneH0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"xgZPUkdwnfwr"},"id":"xgZPUkdwnfwr","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"machine_shape":"hm","gpuType":"V5E1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}