{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","!pip install shap\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import shap\n","import seaborn as sn\n","import sys\n","import sklearn\n","from google.colab import drive\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.utils import resample\n","from scipy.stats import mannwhitneyu\n","from IPython import display\n","from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss, f1_score\n","\n","sn.set(style='whitegrid')\n","pd.set_option('display.max_columns', None)\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)\n","print(\"shap version:\", shap.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["#import your dataset\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"_uAaUKFV7USL","metadata":{"id":"_uAaUKFV7USL"},"outputs":[],"source":["##specify columns to load from your dataset.  We will only load the columns necessary for the score and the variables necessary for the ISS/TRISS comparisons\n","\n","columns_to_load = ['AGEYEARS', 'TOTALGCS', 'SBP'\n","                  , 'TEMPERATURE'\n","                  , 'PULSERATE', 'TRISS', 'TRISS_Death', 'MORTALITY', 'TRAUMATYPE'\n","                  , 'WEIGHT'\n","                  , 'ISS_05', 'NumberOfInjuries',\n","                   'IntracranialVascularInjury','BrainStemInjury','EDH','SAH','SDH','SkullFx','DAI','NeckVascularInjury','ThoracicVascularInjury','AeroDigestiveInjury',\n","                   'CardiacInjury','LungInjury','AbdominalVascular','RibFx','KidneyInjury','StomachInjury','SpleenInjury','UroGenInternalInjury','SCI','SpineFx',\n","                   'UEAmputation','UEVascularInjury','UELongBoneFx','LEVascularInjury','PelvicFx','LEAmputation','PancreasInjury','LELongBoneFx','LiverInjury',\n","                   'ColorectalInjury','SmallBowelInjury','IPH'\n","                   ]"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Import data and specify missing values\n","data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'], usecols=columns_to_load)\n","\n","\n","# Filter out rows where 'TRAUMATYPE' is 26, 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass\n","\n","##explicitly list variables that need to be present for inclusion and drop cases without these\n","##we cannot compare our score to ISS/TRISS without those metrics, and we need our target outcome mortality\n","required_vars = ['ISS_05', 'TRISS_Death', 'MORTALITY']\n","data = data.dropna(subset=required_vars)\n","\n","# Create ShockIndex with the required logic\n","data['ShockIndex'] = np.where(\n","    data['SBP'] == 0, 2.0,  # Case where SBP is 0 â†’ set ShockIndex to 2.0\n","    data['PULSERATE'] / data['SBP']  # Normal calculation\n",")\n","\n","# Set ShockIndex to NaN if PULSERATE or SBP is missing\n","data.loc[data['PULSERATE'].isna() | data['SBP'].isna(), 'ShockIndex'] = np.nan\n","\n","##reset indices of the df\n","data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##verify data appears as intended\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing values\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a datafram of all complications/vars to remove later.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'MORTALITY', 'TRISS'\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, mortality, and give it its own dataframe\n","\n","Y_data = pd.DataFrame()\n","Y_data['MORTALITY'] = data['MORTALITY']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['MORTALITY'] = Y_data['MORTALITY'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##now drop the outcome from our feature space as well as TRISS, since were using 1-TRISS (aka TRISS_Death) and this varibale is now useless\n","X_data = data.drop(columns=['MORTALITY', 'TRISS'])\n","X_data.shape"]},{"cell_type":"code","execution_count":null,"id":"72ac3fa7","metadata":{"id":"72ac3fa7"},"outputs":[],"source":["##ensure no missing outcome data\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"]},{"cell_type":"code","source":["##If we have no missing values here, our data is clean\n","Y_clean=Y_data.copy()"],"metadata":{"id":"0Ur-E_ZOenGg"},"id":"0Ur-E_ZOenGg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##if above check passes, outcome data is now clean\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"],"metadata":{"id":"Qw2kkLmrAeIN"},"id":"Qw2kkLmrAeIN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","##tried different cutoffs for this (33%, 66%), but 50% yielded best results\n","\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#display columns with less than 50% missing that need to be cleaned\n","\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","to_be_cleaned_column_names"]},{"cell_type":"code","source":["# Rename the 'TRAUMATYPE' column to 'Penetrating' and map the values to 0 and 1\n","X_data_new['Penetrating'] = X_data_new['TRAUMATYPE'].map({'Penetrating': 1, 'Blunt': 0})\n","\n","# Drop the old 'TRAUMATYPE' column\n","X_data_new.drop(columns=['TRAUMATYPE'], inplace=True)\n","\n","print(X_data_new.head())"],"metadata":{"id":"v9lgvYMBeSY_"},"id":"v9lgvYMBeSY_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the entire DataFrame without truncation\n","pd.set_option('display.max_columns', None)\n","\n","# Get column names and data types\n","columns_info = []\n","for column_name, dtype in zip(X_data_new.columns, X_data_new.dtypes):\n","    columns_info.append(f\"{column_name}: {dtype}\")\n","\n","formatted_columns_info = \"\\n\".join(columns_info)\n","\n","# Print column names and data types\n","print(\"Column Names and Data Types:\")\n","print(formatted_columns_info)"],"metadata":{"id":"a_7mt7lueXds"},"id":"a_7mt7lueXds","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","\n","##drop any non blunt/penetrating mechanisms\n","try:\n","    X_data_new=X_data_new.drop(['TRAUMATYPE_26', 'TRAUMATYPE_Other/unspecified'], axis=1)\n","except:\n","    pass\n","\n","X_data_new.head()"],"metadata":{"id":"bcHQWaAgeXga"},"id":"bcHQWaAgeXga","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##split into train, test, calibrate sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_data_new, Y_clean, test_size=0.2, random_state=0, stratify=Y_clean)\n","X_train_cal, X_val_cal, Y_train_cal, Y_val_cal = train_test_split(X_train, Y_train, test_size=0.2, random_state=0, stratify=Y_train)"],"metadata":{"id":"oRKxZhxoeM-4"},"id":"oRKxZhxoeM-4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b5ec271e","metadata":{"id":"b5ec271e"},"outputs":[],"source":["##perform median/mode imputation on the inputs vars that are missing\n","for c in to_be_cleaned_column_names:\n","    v = X_train[c]\n","    v_valid = v[~v.isnull()]\n","\n","    if v.dtype == np.dtype('O'):  # Categorical column\n","        mode_value = v_valid.value_counts().index[0]\n","        for df in [X_train, X_test, X_train_cal, X_val_cal]:\n","            df[c] = df[c].fillna(mode_value).astype(object)\n","\n","    else:  # Numeric column\n","        median_value = v_valid.median()\n","        for df in [X_train, X_test, X_train_cal, X_val_cal]:\n","            df[c] = df[c].fillna(median_value)\n"]},{"cell_type":"code","source":["##now for one-hot encoding\n","\n","# Identify categorical columns from X_train only\n","categorical_column = [c for c in X_train_cal.columns if X_train_cal[c].dtype == np.dtype('O')]\n","\n","# Apply pd.get_dummies to training data\n","X_train_cal = pd.get_dummies(X_train_cal, columns=categorical_column, sparse=False)\n","\n","categorical_column"],"metadata":{"id":"DPn3OJ7aguFf"},"id":"DPn3OJ7aguFf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Align test and validation sets to match training set columns\n","X_test = pd.get_dummies(X_test, columns=categorical_column, sparse=False)\n","X_val_cal = pd.get_dummies(X_val_cal, columns=categorical_column, sparse=False)\n","\n","# Ensure same columns across all datasets\n","X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n","X_val_cal = X_val_cal.reindex(columns=X_train.columns, fill_value=0)"],"metadata":{"id":"vPAfgjXUguIQ"},"id":"vPAfgjXUguIQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#verify data appears as intended\n","X_train_cal.head()"],"metadata":{"id":"D5xormBHguLH"},"id":"D5xormBHguLH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"31a5eafa","metadata":{"id":"31a5eafa"},"outputs":[],"source":["##verify no missing data in any split dataset\n","print(X_train_cal.isnull().sum().sum())\n","print(X_test.isnull().sum().sum())\n","print(X_val_cal.isnull().sum().sum())"]},{"cell_type":"code","execution_count":null,"id":"mrWz__0NRJtw","metadata":{"id":"mrWz__0NRJtw"},"outputs":[],"source":["##final list of training columns\n","X_train_cal.columns"]},{"cell_type":"code","source":["#verify data is intended size\n","X_test.shape"],"metadata":{"id":"uxxuPQrkv9da"},"id":"uxxuPQrkv9da","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now with data cleaned, take comparison vars and move them to their own dataframe prior to dropping\n","new_to_drop = ['TRISS_Death', 'ISS_05']\n","\n","X_ISS=pd.DataFrame()\n","X_ISS['ISS']=X_test['ISS_05']\n","\n","X_TRISS=pd.DataFrame()\n","X_TRISS['TRISS']=X_test['TRISS_Death']"],"metadata":{"id":"o5lm5Yi4qbta"},"id":"o5lm5Yi4qbta","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"MkSkefq4RN9D","metadata":{"id":"MkSkefq4RN9D"},"outputs":[],"source":["##now drop those comparison vars from the data that will be fed to the model\n","X_train_cal.drop(columns=new_to_drop, inplace=True)\n","X_test.drop(columns=new_to_drop, inplace=True)\n","X_val_cal.drop(columns=new_to_drop, inplace=True)\n","\n","\n","##store copies of data as tensors\n","X_train_tensor=X_train_cal.copy()\n","Y_train_tensor=Y_train_cal.copy()\n","\n","X_val_tensor=X_val_cal.copy()\n","Y_val_tensor=Y_val_cal.copy()\n","\n","X_test_tensor=X_test.copy()\n","Y_test_tensor=Y_test.copy()"]},{"cell_type":"code","source":["##verify data appears as intended\n","X_test.head()"],"metadata":{"id":"0GKlKCuvKUnw"},"id":"0GKlKCuvKUnw","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"PYoFPWDoRPQW","metadata":{"id":"PYoFPWDoRPQW"},"outputs":[],"source":["##Next step is to normalize data\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train_cal)\n","\n","#normalize the features in the training set\n","X_train_s_cal = scaler.transform(X_train_cal)\n","#normalize the features in the test set\n","print(\"After train/test split, X_test shape:\", X_test.shape)\n","X_test_s = scaler.transform(X_test)\n","print(\"After scaling, X_test_s shape:\", X_test_s.shape)\n","#normalize the features in the val set\n","X_val_s_cal = scaler.transform(X_val_cal)"]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##now, fit model with hyperparameters based on other Jupyternotebook optimization\n","model_best_gb = xgb.XGBClassifier(random_state=0, colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0)\n","model_best_gb.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"L9Wj3__pbnO6","metadata":{"id":"L9Wj3__pbnO6"},"outputs":[],"source":["# Get predicted probabilities for test set (evaluate model)\n","from sklearn.metrics import roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix\n","\n","y_prob_gbo_mtp = model_best_gb.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC on test set\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"]},{"cell_type":"code","execution_count":null,"id":"A82t-nm2qKYQ","metadata":{"id":"A82t-nm2qKYQ"},"outputs":[],"source":["# Calibrate the model on the validation set\n","calibrated_model = CalibratedClassifierCV(estimator=model_best_gb, method='isotonic', cv='prefit')\n","calibrated_model.fit(X_val_s_cal, Y_val_cal)"]},{"cell_type":"code","execution_count":null,"id":"AT5kTGp8qK0k","metadata":{"id":"AT5kTGp8qK0k"},"outputs":[],"source":["# Get predicted probabilities for test set (evaluate model)\n","y_prob_gbo_mtp = calibrated_model.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC on test set (calibrated)\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"]},{"cell_type":"code","source":["##ensure true labels and predicted probabilities are in flat, clean pandas Series with aligned indices to allow for descriptive stats\n","y_true_series = pd.Series(Y_test.squeeze()).reset_index(drop=True)\n","y_prob_series = pd.Series(y_prob_gbo_mtp).reset_index(drop=True)\n","\n","##get descriptive stats for population MLISS scoers\n","mean_prob_all = y_prob_series.mean()\n","median_prob_all = y_prob_series.median()\n","mean_prob_label1 = y_prob_series[y_true_series == 1].mean()\n","mean_prob_label0 = y_prob_series[y_true_series == 0].mean()\n","median_prob_label1 = y_prob_series[y_true_series == 1].median()\n","median_prob_label0 = y_prob_series[y_true_series == 0].median()\n","\n","print(f\"Mean predicted probability (all): {mean_prob_all:.4f}\")\n","print(f\"Mean predicted probability (label=1): {mean_prob_label1:.4f}\")\n","print(f\"Mean predicted probability (label=0): {mean_prob_label0:.4f}\")\n","\n","print(f\"Median predicted probability (all): {median_prob_all:.4f}\")\n","print(f\"Median predicted probability (label=1): {median_prob_label1:.4f}\")\n","print(f\"Median predicted probability (label=0): {median_prob_label0:.4f}\")\n"],"metadata":{"id":"PIMYitYT6aoL"},"id":"PIMYitYT6aoL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a DF of true labels and predicted mortalities for more descriptive stats\n","df = pd.DataFrame({\n","    'y_true': Y_test['MORTALITY'],\n","    'y_prob': y_prob_gbo_mtp\n","})\n","\n","# Q1 and Q3 for overall cohort\n","q1_all = df['y_prob'].quantile(0.25)\n","q3_all = df['y_prob'].quantile(0.75)\n","\n","# Q1 and Q3 for label = 1\n","q1_label1 = df[df['y_true'] == 1]['y_prob'].quantile(0.25)\n","q3_label1 = df[df['y_true'] == 1]['y_prob'].quantile(0.75)\n","\n","# Q1 and Q3 for label = 0\n","q1_label0 = df[df['y_true'] == 0]['y_prob'].quantile(0.25)\n","q3_label0 = df[df['y_true'] == 0]['y_prob'].quantile(0.75)\n","\n","# Print results\n","print(f\"Overall:     Q1 = {q1_all:.4f}, Q3 = {q3_all:.4f}\")\n","print(f\"Label = 1:   Q1 = {q1_label1:.4f}, Q3 = {q3_label1:.4f}\")\n","print(f\"Label = 0:   Q1 = {q1_label0:.4f}, Q3 = {q3_label0:.4f}\")"],"metadata":{"id":"ETsduYFi9va8"},"id":"ETsduYFi9va8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##evaluate whether MLISS score distributions and differnet between surivvors and deceased patients with Mann Whitney U\n","\n","# Split into two groups\n","probs_label1 = df[df['y_true'] == 1]['y_prob']\n","probs_label0 = df[df['y_true'] == 0]['y_prob']\n","\n","# Perform Mann-Whitney U test\n","u_statistic, p_value = mannwhitneyu(probs_label1, probs_label0, alternative='two-sided')\n","\n","print(f\"Mann-Whitney U statistic: {u_statistic:.4f}\")\n","print(f\"P-value: {p_value:.4f}\")"],"metadata":{"id":"GIpr8uW3Yx5b"},"id":"GIpr8uW3Yx5b","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"Ks5tYVvyqOD3","metadata":{"id":"Ks5tYVvyqOD3"},"outputs":[],"source":["# Get predicted probabilities for test set (evaluate model)\n","y_prob_gbo = calibrated_model.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC on test set (calibrated)\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo)\n","print(f\"AUROC on the test set: {auroc_gbo}\")\n","\n","\n","# Calculate ROC curve\n","y_pred_prob_gbo = calibrated_model.predict_proba(X_test_s)[:, 1]\n","fpr_gbo, tpr_gbo, thresholds = roc_curve(Y_test, y_pred_prob_gbo)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_gbo = auc(fpr_gbo, tpr_gbo)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_gbo, tpr_gbo, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_gbo:.3f})')\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Calculate precision and recall values\n","precision, recall, thresholds = precision_recall_curve(Y_test, y_prob_gbo_mtp)\n","\n","# Calculate area under the precision-recall curve\n","pr_auc = auc(recall, precision)\n","\n","# Plot the precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label=f'PR curve (AUC={pr_auc:.3f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.show()\n","\n","# Choose a custom threshold (e.g., 0.3)\n","##this value yielded the highest value for accuracy\n","custom_threshold = 0.5\n","\n","# Apply the custom threshold to make binary predictions\n","y_pred_custom_threshold = (y_prob_gbo > custom_threshold).astype(int)\n","\n","num_positive_cases = sum(y_pred_custom_threshold)\n","num_negative_cases = len(y_pred_custom_threshold) - num_positive_cases\n","print(\"Number of Cases Classified as Positive:\", num_positive_cases)\n","print(\"Number of Cases Classified as Negative:\", num_negative_cases)\n","\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(Y_test, y_pred_custom_threshold)\n","\n","print('CONFUSION MATRIX')\n","print('-----------')\n","print('|',conf_matrix[0,0],'|', conf_matrix[0,1], '|')\n","print('-----------')\n","print('|',conf_matrix[1,0],'|', conf_matrix[1,1], '|')\n","\n","# Calculate false positive rate (FPR) and false negative rate (FNR)\n","\n","falseNeg = conf_matrix[1, 0]\n","falsePos = conf_matrix[0, 1]\n","trueNeg = conf_matrix[0, 0]\n","truePos = conf_matrix[1, 1]\n","actualPos = truePos + falseNeg\n","actualNeg = trueNeg + falsePos\n","predPos = truePos + falsePos\n","predNeg = trueNeg + falseNeg\n","\n","\n","fpr = falsePos / actualNeg\n","fnr = falseNeg / actualPos\n","tpr = truePos / actualPos\n","tnr = trueNeg / actualNeg\n","accuracy = (truePos + trueNeg) / (actualPos + actualNeg)\n","ppV = truePos / predPos\n","npV = trueNeg / predNeg\n","\n","brier_score = brier_score_loss(Y_test, y_pred_prob_gbo)\n","\n","print('Actual Positives:', actualPos)\n","print('Actual Negatives:', actualNeg)\n","print(\"False Positive Rate (FPR):\", fpr)\n","print(\"False Negative Rate (FNR):\", fnr)\n","print(\"True Positive Rate (TPR) (Recall/Sensitivity):\", tpr) ##right\n","print(\"True Negative Rate (TNR) (specificity):\", tnr)\n","print('Accuracy:', accuracy) ##right\n","print('Precision (PPV),', ppV) ##right\n","print('NPV', npV)\n","print(\"Brier Score:\", brier_score)\n","\n","# Convert probabilities to binary labels using the threshold\n","y_pred_binary10 = (y_prob_gbo > 0.1).astype(int)\n","y_pred_binary20 = (y_prob_gbo > 0.2).astype(int)\n","y_pred_binary30 = (y_prob_gbo > 0.3).astype(int)\n","y_pred_binary40 = (y_prob_gbo > 0.4).astype(int)\n","y_pred_binary50 = (y_prob_gbo > 0.5).astype(int)\n","y_pred_binary60 = (y_prob_gbo > 0.6).astype(int)\n","y_pred_binary70 = (y_prob_gbo > 0.7).astype(int)\n","y_pred_binary80 = (y_prob_gbo > 0.8).astype(int)\n","y_pred_binary90 = (y_prob_gbo > 0.9).astype(int)\n","y_pred_binary = (y_prob_gbo > custom_threshold).astype(int)\n","\n","# Compute F1 score\n","f1 = f1_score(Y_test, y_pred_binary)\n","f1_10 = f1_score(Y_test, y_pred_binary10)\n","f1_20 = f1_score(Y_test, y_pred_binary20)\n","f1_30 = f1_score(Y_test, y_pred_binary30)\n","f1_40 = f1_score(Y_test, y_pred_binary40)\n","f1_50 = f1_score(Y_test, y_pred_binary50)\n","f1_60 = f1_score(Y_test, y_pred_binary60)\n","f1_70 = f1_score(Y_test, y_pred_binary70)\n","f1_80 = f1_score(Y_test, y_pred_binary80)\n","f1_90 = f1_score(Y_test, y_pred_binary90)\n","\n","print(\"F1 Score:\", f1)\n","print(\"F1-10 Score:\", f1_10)\n","print(\"F1-20 Score:\", f1_20)\n","print(\"F1-30 Score:\", f1_30)\n","print(\"F1-40 Score:\", f1_40)\n","print(\"F1-50 Score:\", f1_50)\n","print(\"F1-60 Score:\", f1_60)\n","print(\"F1-70 Score:\", f1_70)\n","print(\"F1-80 Score:\", f1_80)\n","print(\"F1-90 Score:\", f1_90)"]},{"cell_type":"code","source":["##evaluate TRISS as predictive tool\n","predicted_prob_triss = X_TRISS['TRISS']  # This should be probabilities (e.g., 0.8, 0.3)\n","true_label = Y_test['MORTALITY']          # This should be binary labels (e.g., 1 for death, 0 for survival)\n","\n","# Calculate FPR, TPR, and thresholds\n","fpr_triss, tpr_triss, thresholds_triss = roc_curve(true_label, predicted_prob_triss)\n","\n","# Calculate the area under the ROC curve\n","roc_auc_triss = auc(fpr_triss, tpr_triss)\n","\n","# Plot the ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_triss, tpr_triss, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc_triss:.3f}')\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--',label='Random')  # Dashed diagonal line for random model\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend()\n","plt.show()\n","\n","\n","\n","# Print the ROC AUC value\n","print(f'ROC AUC: {roc_auc_triss:.3f}')"],"metadata":{"id":"oMehB59t_PYj"},"id":"oMehB59t_PYj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit logistic regression to map ISS scores to probabilities\n","lr_iss = LogisticRegression()\n","lr_iss.fit(X_ISS.values.reshape(-1,1), Y_test)\n","\n","# Predict probabilities\n","iss_probs = lr_iss.predict_proba(X_ISS.values.reshape(-1,1))[:,1]"],"metadata":{"id":"vMqbW9khgmdU"},"id":"vMqbW9khgmdU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##evaluate ISS as predictive tool\n","\n","predicted_prob_iss = iss_probs\n","true_label = Y_test['MORTALITY']   # This should be the binary outcome variable (0 or 1)\n","\n","# Calculate the FPR, TPR, and thresholds\n","fpr_iss, tpr_iss, thresholds_iss = roc_curve(true_label, predicted_prob_iss)\n","\n","# Calculate the area under the ROC curve (AUROC)\n","roc_auc_iss = auc(fpr_iss, tpr_iss)\n","\n","# Plot the ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_triss, tpr_triss, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc_iss:.3f}')\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--',label='Random')  # Dashed diagonal line for random model\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend()\n","plt.show()\n","\n","# Print the AUROC value\n","print(f'ROC AUC: {roc_auc_iss:.3f}')"],"metadata":{"id":"j60nh15i_Pc6"},"id":"j60nh15i_Pc6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot ROC curves of all 3\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_gbo, tpr_gbo, color='b', lw=2, label=f'ROC curve (area = {roc_auc_gbo:.3f})')\n","plt.plot(fpr_triss, tpr_triss, color='green', lw=2, label=f'ROC AUC TRISS = {roc_auc_triss:.3f}')\n","plt.plot(fpr_iss, tpr_iss, color='darkorange', lw=2, label=f'ROC AUC ISS = {roc_auc_iss:.3f}')\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')"],"metadata":{"id":"8ff_LH5Fabxj"},"id":"8ff_LH5Fabxj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now we evaluate calibration of all 3 metrics (MLISS, TRISS, ISS)\n","\n","def bootstrap_calibration_curve(y_true, y_prob, n_bins=10, n_boot=1000, random_state=None):\n","    \"\"\"\n","    1) Compute the original bin-based calibration curve.\n","    2) Bootstrap the dataset n_boot times, each time recalculating the bin-based\n","       fraction of positives (prob_true) and storing it.\n","    3) Return the original curve + 95% CI per bin (based on 2.5 and 97.5 percentiles).\n","    \"\"\"\n","    # -----------------------------\n","    # Original calibration curve\n","    # -----------------------------\n","    # prob_true_orig, prob_pred_orig = calibration_curve(...) does binning internally.\n","    # But we want to fix n_bins and ensure consistent binning across bootstraps.\n","    # We'll do a manual binning approach here to keep consistent bin boundaries.\n","\n","    # Define bin edges (equally spaced from 0 to 1)\n","    bin_edges = np.linspace(0, 1, n_bins + 1)\n","    # Digitize predicted probabilities\n","    bin_indices = np.digitize(y_prob, bin_edges) - 1\n","    bin_indices[bin_indices == n_bins] = n_bins - 1  # cap any == n_bins to last bin\n","\n","    # Prepare arrays to hold the original bin stats\n","    prob_pred_orig = np.zeros(n_bins)\n","    prob_true_orig = np.zeros(n_bins)\n","    counts_in_bin = np.zeros(n_bins, dtype=int)\n","\n","    # Fill in the stats for each bin\n","    for i in range(n_bins):\n","        mask = (bin_indices == i)\n","        counts_in_bin[i] = np.sum(mask)\n","        if counts_in_bin[i] > 0:\n","            prob_pred_orig[i] = np.mean(y_prob[mask])   # mean predicted prob in this bin\n","            prob_true_orig[i] = np.mean(y_true[mask])   # fraction of positives (actual)\n","        else:\n","            # If bin is empty, set to NaN\n","            prob_pred_orig[i] = np.nan\n","            prob_true_orig[i] = np.nan\n","\n","    # Remove empty bins (NaN) from the original arrays\n","    valid_mask = ~np.isnan(prob_pred_orig)\n","    prob_pred_orig = prob_pred_orig[valid_mask]\n","    prob_true_orig = prob_true_orig[valid_mask]\n","\n","    # -----------------------------\n","    # Bootstrap to get CIs\n","    # -----------------------------\n","    rng = np.random.RandomState(random_state) if random_state else np.random\n","\n","    # We'll store the fraction of positives (prob_true) for each bin in each bootstrap\n","    # but only for the bins that were valid in the original data\n","    boot_prob_true = np.zeros((n_boot, sum(valid_mask)))\n","\n","    n_data = len(y_true)\n","    data_idx = np.arange(n_data)\n","\n","    for b in range(n_boot):\n","        # Sample with replacement\n","        sample_indices = rng.randint(0, n_data, size=n_data)\n","        y_true_b = y_true[sample_indices]\n","        y_prob_b = y_prob[sample_indices]\n","\n","        # Repeat the binning steps\n","        bin_indices_b = np.digitize(y_prob_b, bin_edges) - 1\n","        bin_indices_b[bin_indices_b == n_bins] = n_bins - 1\n","\n","        prob_true_b = np.zeros(n_bins)\n","        for i in range(n_bins):\n","            mask_b = (bin_indices_b == i)\n","            if np.sum(mask_b) > 0:\n","                prob_true_b[i] = np.mean(y_true_b[mask_b])\n","            else:\n","                prob_true_b[i] = np.nan\n","\n","        # filter to only valid bins\n","        prob_true_b = prob_true_b[valid_mask]\n","        boot_prob_true[b, :] = prob_true_b\n","\n","    # Compute 2.5th and 97.5th percentile per bin (column-wise)\n","    lower_ci = np.nanpercentile(boot_prob_true, 2.5, axis=0)\n","    upper_ci = np.nanpercentile(boot_prob_true, 97.5, axis=0)\n","\n","    return prob_pred_orig, prob_true_orig, lower_ci, upper_ci\n","\n","# -----------------------------------------------------------\n","# Now apply the function\n","# -----------------------------------------------------------\n","\n","# === Input arrays ===\n","y_true = np.array(Y_test)\n","y_prob_gbo = np.array(y_prob_gbo)       # ML model\n","y_prob_triss = np.array(X_TRISS['TRISS'])  # or however you're storing TRISS scores\n","y_prob_iss = np.array(iss_probs)        # or however you're storing ISS scores\n","\n","# === Helper function to compute sorted calibration data + CIs ===\n","def get_bootstrap_calibration_data(y_true, y_prob, label, color, n_bins=10, n_boot=1000, random_state=42):\n","    prob_pred, prob_true, lower_ci, upper_ci = bootstrap_calibration_curve(\n","        y_true, y_prob, n_bins=n_bins, n_boot=n_boot, random_state=random_state\n","    )\n","    sort_idx = np.argsort(prob_pred)\n","    return {\n","        \"x\": prob_pred[sort_idx],\n","        \"y\": prob_true[sort_idx],\n","        \"lower\": lower_ci[sort_idx],\n","        \"upper\": upper_ci[sort_idx],\n","        \"label\": label,\n","        \"color\": color\n","    }\n","\n","# === Get calibration data ===\n","calib_gbo = get_bootstrap_calibration_data(y_true, y_prob_gbo, label=\"ML Model\", color='b')\n","calib_triss = get_bootstrap_calibration_data(y_true, y_prob_triss, label=\"TRISS\", color='g')\n","calib_iss = get_bootstrap_calibration_data(y_true, y_prob_iss, label=\"ISS\", color='darkorange')\n","\n","# === Compute Brier scores ===\n","brier_gbo = brier_score_loss(y_true, y_prob_gbo)\n","brier_triss = brier_score_loss(y_true, y_prob_triss)\n","brier_iss = brier_score_loss(y_true, y_prob_iss)\n","\n","print(f\"Brier Score - ML Model: {brier_gbo:.4f}\")\n","print(f\"Brier Score - TRISS:    {brier_triss:.4f}\")\n","print(f\"Brier Score - ISS:      {brier_iss:.4f}\")\n","\n","# === Plot Reliability Diagram ===\n","plt.figure(figsize=(8, 6))\n","plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n","\n","for calib in [calib_gbo, calib_triss, calib_iss]:\n","    plt.plot(calib[\"x\"], calib[\"y\"], marker='o', label=calib[\"label\"], color=calib[\"color\"])\n","    plt.fill_between(calib[\"x\"], calib[\"lower\"], calib[\"upper\"], color=calib[\"color\"], alpha=0.2)\n","\n","plt.xlabel('Mean Predicted Probability')\n","plt.ylabel('Observed Mortality Rate')\n","plt.title('Reliability Diagram with 95% CI â€“ All Methods')\n","plt.legend(loc='best')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"BAnZCmBnWj5I"},"id":"BAnZCmBnWj5I","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decision Curve analysis\n","# ========================================\n","\n","##first define function to calculate net benefit\n","def net_benefit(y_true, y_prob, thresholds):\n","    \"\"\"\n","    NetBenefit = (TP/N) - (FP/N)*(threshold/(1-threshold))\n","    \"\"\"\n","    N = len(y_true)\n","    NB = []\n","    for t in thresholds:\n","        y_pred = (y_prob >= t).astype(int)\n","        TP = np.sum((y_true == 1) & (y_pred == 1))\n","        FP = np.sum((y_true == 0) & (y_pred == 1))\n","        if t == 1.0:\n","            nb_t = 0\n","        else:\n","            nb_t = (TP / N) - (FP / N) * (t / (1 - t))\n","        NB.append(nb_t)\n","    return NB\n","\n","# Thresholds for decision curve\n","decision_thresholds = np.linspace(0.0, 1.0, 101)\n","\n","# Make sure y_true is an array\n","y_true_array = np.asarray(Y_test).flatten()\n","\n","# Net benefit for MLISS\n","NB_model = net_benefit(y_true_array, y_prob_gbo, decision_thresholds)\n","\n","# # Net benefit for ISS (ISS predictions: X_ISS.values)\n","NB_ISS = net_benefit(y_true_array, iss_probs, decision_thresholds)\n","\n","# Net benefit for TRISS (TRISS predictions: X_TRISS.values)\n","NB_TRISS = net_benefit(y_true_array, X_TRISS.values.flatten(), decision_thresholds)\n","\n","# Net benefit for treat all and treat none (baseline strategies)\n","N = len(Y_test)\n","prevalence = np.mean(Y_test)  # fraction of positives\n","treat_all_nb = []\n","for t in decision_thresholds:\n","    if t == 1.0:\n","        treat_all_nb.append(0)\n","    else:\n","        treat_all_nb.append(prevalence - (1 - prevalence)*(t/(1-t)))\n","\n","treat_none_nb = np.zeros_like(decision_thresholds)\n","\n","# now plot DCA\n","plt.figure(figsize=(8, 6))\n","plt.plot(decision_thresholds, NB_model, label='New ML Model', color='b')\n","plt.plot(decision_thresholds, NB_ISS, label='ISS', color='darkorange')\n","plt.plot(decision_thresholds, NB_TRISS, label='TRISS', color='g')\n","plt.plot(decision_thresholds, treat_all_nb, label='Treat All', color='red', linestyle='--')\n","plt.plot(decision_thresholds, treat_none_nb, label='Treat None', color='grey', linestyle=':')\n","\n","plt.xlabel('Threshold Probability')\n","plt.ylabel('Net Benefit')\n","plt.title('Decision Curve Analysis')\n","plt.legend(loc='best')\n","plt.ylim([-0.04, 0.04])\n","plt.xlim([0, 1.0])\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"il_KC9t4oyhW"},"id":"il_KC9t4oyhW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ========================================\n","#  Confusion Matrix Stats @ threshold=0.5, similar to above derivation\n","# ========================================\n","custom_threshold = 0.5\n","y_pred_custom_threshold = (y_prob_gbo >= custom_threshold).astype(int)\n","\n","conf_matrix = confusion_matrix(Y_test, y_pred_custom_threshold)\n","falseNeg, falsePos, trueNeg, truePos = conf_matrix[1, 0], conf_matrix[0, 1], conf_matrix[0, 0], conf_matrix[1, 1]\n","actualPos, actualNeg = truePos + falseNeg, trueNeg + falsePos\n","predPos, predNeg = truePos + falsePos, trueNeg + falseNeg\n","\n","fpr = falsePos / actualNeg if actualNeg else 0\n","fnr = falseNeg / actualPos if actualPos else 0\n","tpr = truePos / actualPos if actualPos else 0\n","tnr = trueNeg / actualNeg if actualNeg else 0\n","accuracy = (truePos + trueNeg) / (actualPos + actualNeg) if (actualPos + actualNeg) else 0\n","ppV = truePos / predPos if predPos != 0 else 0\n","npV = trueNeg / predNeg if predNeg != 0 else 0\n","\n","brier_score = brier_score_loss(Y_test, y_prob_gbo)\n","\n","print(\"\\n=== Threshold = 0.50 ===\")\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","print(f\"False Positive Rate (FPR): {fpr:.3f}\")\n","print(f\"False Negative Rate (FNR): {fnr:.3f}\")\n","print(f\"True Positive Rate (TPR/Recall): {tpr:.3f}\")\n","print(f\"True Negative Rate (TNR/Specificity): {tnr:.3f}\")\n","print(f\"Accuracy: {accuracy:.3f}\")\n","print(f\"Precision (PPV): {ppV:.3f}\")\n","print(f\"Negative Predictive Value (NPV): {npV:.3f}\")\n","print(f\"Brier Score: {brier_score:.4f}\")\n","\n","# ========================================\n","# Find Threshold that Maximizes F1 (0.01 increments)\n","# ========================================\n","candidate_thresholds = np.linspace(0, 1, 101)  # from 0.00 to 1.00 in 0.01 steps\n","best_f1 = -1.0\n","best_f1_threshold = 0.0\n","\n","for t in candidate_thresholds:\n","    y_pred = (y_prob_gbo >= t).astype(int)\n","    f1_val = f1_score(Y_test, y_pred)\n","    if f1_val > best_f1:\n","        best_f1 = f1_val\n","        best_f1_threshold = t\n","\n","print(f\"\\nOptimal threshold for max F1 (nearest hundredth): {best_f1_threshold:.2f}\")\n","print(f\"Max F1 Score at that threshold: {best_f1:.3f}\")\n","\n","# ========================================\n","# Find Threshold for >=90% Specificity (0.01 increments)\n","# ========================================\n","tnr_90_thresh = None\n","\n","for t in candidate_thresholds:\n","    y_pred = (y_prob_gbo >= t).astype(int)\n","    cm = confusion_matrix(Y_test, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","    actual_neg = tn + fp\n","    if actual_neg == 0:\n","        continue\n","\n","    specificity = tn / actual_neg  # TNR\n","    if specificity >= 0.90:\n","        tnr_90_thresh = t\n","        break  # first threshold that achieves TNR >= 0.90\n","\n","if tnr_90_thresh is not None:\n","    print(f\"\\nThreshold achieving >=90% specificity (nearest hundredth): {tnr_90_thresh:.2f}\")\n","    # Recompute confusion matrix & TPR at that threshold\n","    y_pred_90sp = (y_prob_gbo >= tnr_90_thresh).astype(int)\n","    cm_90 = confusion_matrix(Y_test, y_pred_90sp)\n","    tn_90, fp_90, fn_90, tp_90 = cm_90.ravel()\n","    tnr_val_90 = tn_90 / (tn_90 + fp_90) if (tn_90 + fp_90) else 0\n","    tpr_val_90 = tp_90 / (tp_90 + fn_90) if (tp_90 + fn_90) else 0\n","    print(f\"Specificity (TNR) at {tnr_90_thresh:.2f}: {tnr_val_90:.3f}\")\n","    print(f\"Sensitivity (TPR) at {tnr_90_thresh:.2f}: {tpr_val_90:.3f}\")\n","else:\n","    print(\"\\nNo threshold found where TNR >= 0.90 in [0.00, 1.00].\")\n","# ========================================\n","# Define a Function to Compute Metrics\n","# ========================================\n","def compute_metrics(y_true, y_prob, threshold):\n","    \"\"\"\n","    Computes performance metrics at a given threshold.\n","\n","    Parameters:\n","    - y_true: True binary labels\n","    - y_prob: Predicted probabilities for the positive class\n","    - threshold: Threshold to convert probabilities to binary predictions\n","\n","    Returns:\n","    - metrics_dict: Dictionary containing performance metrics\n","    \"\"\"\n","    y_pred = (y_prob >= threshold).astype(int)\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","\n","    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) else 0\n","    sensitivity = tp / (tp + fn) if (tp + fn) else 0  # TPR\n","    specificity = tn / (tn + fp) if (tn + fp) else 0  # TNR\n","    precision = tp / (tp + fp) if (tp + fp) else 0      # PPV\n","    npv = tn / (tn + fn) if (tn + fn) else 0            # NPV\n","    f1 = f1_score(y_true, y_pred) if (tp + fp + fn) else 0\n","\n","    metrics_dict = {\n","        'Accuracy': accuracy,\n","        'Sensitivity (TPR)': sensitivity,\n","        'Specificity (TNR)': specificity,\n","        'Precision (PPV)': precision,\n","        'Negative Predictive Value (NPV)': npv,\n","        'F1 Score': f1\n","    }\n","\n","    return metrics_dict\n","\n","# ========================================\n","# Create and Print Performance Tables\n","# ========================================\n","# Initialize an empty list to store metric dictionaries\n","performance_metrics = []\n","\n","# Thresholds to report\n","thresholds_to_report = {\n","    'Max F1 Score Threshold': best_f1_threshold,\n","    '90% Specificity Threshold': tnr_90_thresh\n","}\n","\n","# Compute metrics for each threshold\n","for desc, thresh in thresholds_to_report.items():\n","    if thresh is not None:\n","        metrics = compute_metrics(Y_test, y_prob_gbo, thresh)\n","        metrics['Threshold'] = f\"{thresh:.2f}\"\n","        performance_metrics.append(metrics)\n","    else:\n","        print(f\"\\n{desc} is not available as no threshold achieved >=90% specificity.\")\n","\n","# Create a DataFrame\n","df_performance = pd.DataFrame(performance_metrics)\n","\n","# Reorder columns for better readability\n","df_performance = df_performance[['Threshold', 'Accuracy', 'Sensitivity (TPR)',\n","                                 'Specificity (TNR)', 'Precision (PPV)',\n","                                 'Negative Predictive Value (NPV)', 'F1 Score']]\n","\n","# Set descriptive index\n","df_performance.index = thresholds_to_report.keys()\n","\n","# Display the tables\n","print(\"\\n=== Performance Metrics at Selected Thresholds ===\")\n","print(df_performance.to_string(float_format=\"{:.3f}\".format))\n","\n","# ISS (already probability)\n","y_pred_iss = (iss_probs.flatten() >= 0.27).astype(int)\n","f1_iss = f1_score(Y_test, y_pred_iss)\n","\n","# TRISS (already probability)\n","y_pred_triss = (X_TRISS.values.flatten() >= 0.27).astype(int)\n","f1_triss = f1_score(Y_test, y_pred_triss)\n","\n","# Now you have best threshold and F1 for your ML model\n","print(\"\\n=== F1 Scores ===\")\n","print(f\"F1 Score (Your ML Model @ optimal threshold): {best_f1:.3f}\")\n","print(f\"F1 Score (TRISS @ 0.5 threshold): {f1_triss:.3f}\")\n","print(f\"F1 Score (ISS Probs @ 0.5 threshold): {f1_iss:.3f}\")\n"],"metadata":{"id":"j9ITeL9sOkzS"},"id":"j9ITeL9sOkzS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ======================================\n","# Find Optimal Thresholds for Max F1 and ~90% Specificity\n","# ======================================\n","\n","candidate_thresholds = np.linspace(0, 1, 101)  # from 0.00 to 1.00 in 0.01 steps\n","\n","# Initialize trackers\n","best_f1_iss = -1.0\n","best_f1_threshold_iss = 0.0\n","spec90_threshold_iss = None\n","closest_spec90_diff_iss = float(\"inf\")\n","\n","best_f1_triss = -1.0\n","best_f1_threshold_triss = 0.0\n","spec90_threshold_triss = None\n","closest_spec90_diff_triss = float(\"inf\")\n","\n","# Function to compute specificity\n","def get_specificity(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp)\n","\n","# 1. ISS\n","for t in candidate_thresholds:\n","    y_pred = (iss_probs.flatten() >= t).astype(int)\n","    f1_val = f1_score(Y_test, y_pred)\n","    spec_val = get_specificity(Y_test, y_pred)\n","\n","    # Max F1\n","    if f1_val > best_f1_iss:\n","        best_f1_iss = f1_val\n","        best_f1_threshold_iss = t\n","\n","    # Closest to 90% specificity\n","    if abs(spec_val - 0.90) < closest_spec90_diff_iss:\n","        closest_spec90_diff_iss = abs(spec_val - 0.90)\n","        spec90_threshold_iss = t\n","\n","# 2. TRISS\n","for t in candidate_thresholds:\n","    y_pred = (X_TRISS.values.flatten() >= t).astype(int)\n","    f1_val = f1_score(Y_test, y_pred)\n","    spec_val = get_specificity(Y_test, y_pred)\n","\n","    # Max F1\n","    if f1_val > best_f1_triss:\n","        best_f1_triss = f1_val\n","        best_f1_threshold_triss = t\n","\n","    # Closest to 90% specificity\n","    if abs(spec_val - 0.90) < closest_spec90_diff_triss:\n","        closest_spec90_diff_triss = abs(spec_val - 0.90)\n","        spec90_threshold_triss = t\n","\n","# ========================================\n","# Print Results\n","# ========================================\n","print(\"\\n=== Optimal Thresholds ===\")\n","print(f\"ISS - Best F1 = {best_f1_iss:.3f} at Threshold = {best_f1_threshold_iss:.2f}\")\n","print(f\"ISS - ~90% Specificity Threshold = {spec90_threshold_iss:.2f}\")\n","\n","print(f\"TRISS - Best F1 = {best_f1_triss:.3f} at Threshold = {best_f1_threshold_triss:.2f}\")\n","print(f\"TRISS - ~90% Specificity Threshold = {spec90_threshold_triss:.2f}\")\n"],"metadata":{"id":"TGrfY1sPRHHD"},"id":"TGrfY1sPRHHD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ========================================\n","# Expanded Performance Metrics for ML, ISS, and TRISS at Both Thresholds\n","# ========================================\n","\n","# 1. Compute performance metrics at each threshold\n","performance_metrics_all = []\n","\n","# ML Model\n","for desc, thresh in thresholds_to_report.items():\n","    if thresh is not None:\n","        metrics = compute_metrics(Y_test, y_prob_gbo, thresh)\n","        metrics['Threshold'] = f\"{thresh:.2f}\"\n","        metrics['Model'] = 'ML Model'\n","        metrics['Threshold Type'] = desc\n","        performance_metrics_all.append(metrics)\n","\n","# ISS Model at both thresholds\n","# Max F1\n","metrics_iss_f1 = compute_metrics(Y_test, iss_probs.flatten(), best_f1_threshold_iss)\n","metrics_iss_f1['Threshold'] = f\"{best_f1_threshold_iss:.2f}\"\n","metrics_iss_f1['Model'] = 'ISS'\n","metrics_iss_f1['Threshold Type'] = 'Max F1 Score Threshold'\n","performance_metrics_all.append(metrics_iss_f1)\n","\n","# ~90% Specificity\n","metrics_iss_spec90 = compute_metrics(Y_test, iss_probs.flatten(), spec90_threshold_iss)\n","metrics_iss_spec90['Threshold'] = f\"{spec90_threshold_iss:.2f}\"\n","metrics_iss_spec90['Model'] = 'ISS'\n","metrics_iss_spec90['Threshold Type'] = '90% Specificity Threshold'\n","performance_metrics_all.append(metrics_iss_spec90)\n","\n","# TRISS Model at both thresholds\n","# Max F1\n","metrics_triss_f1 = compute_metrics(Y_test, X_TRISS.values.flatten(), best_f1_threshold_triss)\n","metrics_triss_f1['Threshold'] = f\"{best_f1_threshold_triss:.2f}\"\n","metrics_triss_f1['Model'] = 'TRISS'\n","metrics_triss_f1['Threshold Type'] = 'Max F1 Score Threshold'\n","performance_metrics_all.append(metrics_triss_f1)\n","\n","# ~90% Specificity\n","metrics_triss_spec90 = compute_metrics(Y_test, X_TRISS.values.flatten(), spec90_threshold_triss)\n","metrics_triss_spec90['Threshold'] = f\"{spec90_threshold_triss:.2f}\"\n","metrics_triss_spec90['Model'] = 'TRISS'\n","metrics_triss_spec90['Threshold Type'] = '90% Specificity Threshold'\n","performance_metrics_all.append(metrics_triss_spec90)\n","\n","# ========================================\n","# Create and display Combined DataFrame\n","# ========================================\n","df_performance_all = pd.DataFrame(performance_metrics_all)\n","\n","# Reorder columns\n","df_performance_all = df_performance_all[['Model', 'Threshold Type', 'Threshold', 'Accuracy',\n","                                         'Sensitivity (TPR)', 'Specificity (TNR)',\n","                                         'Precision (PPV)', 'Negative Predictive Value (NPV)',\n","                                         'F1 Score']]\n","\n","# Display results\n","print(\"\\n=== Expanded Performance Metrics ===\")\n","print(df_performance_all.to_string(float_format=\"{:.3f}\".format))\n"],"metadata":{"id":"Q7EzWM2xRqqq"},"id":"Q7EzWM2xRqqq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Brier score for ISS\n","brier_score_iss = brier_score_loss(Y_test, iss_probs.flatten())\n","print(f\"Brier Score (ISS): {brier_score_iss:.4f}\")\n","\n","# Brier score for TRISS\n","brier_score_triss = brier_score_loss(Y_test, X_TRISS.values.flatten())\n","print(f\"Brier Score (TRISS): {brier_score_triss:.4f}\")"],"metadata":{"id":"9AjwzJ8eX3G0"},"id":"9AjwzJ8eX3G0","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"mwuiRojPF6ah","metadata":{"id":"mwuiRojPF6ah"},"outputs":[],"source":["##now, comput 95% confidence interval based on 1000-sample bootstrapping\n","\n","# Initialize your classifier (replace with your actual classifier)\n","classifier = model_best_gb\n","\n","# Compute the AUROC on the original test set\n","original_auroc = auroc_gbo\n","\n","\n","\n","# Number of bootstrap samples\n","n_bootstrap_samples = 1000\n","\n","# Initialize an array to store bootstrapped AUROC values\n","bootstrap_aurocs = np.zeros(n_bootstrap_samples)\n","\n","# Perform bootstrapping\n","for i in range(n_bootstrap_samples):\n","    # Generate a bootstrap sample\n","    bootstrap_indices = np.random.choice(len(Y_test_tensor), len(Y_test_tensor), replace=True)\n","    y_bootstrap = y_prob_gbo_mtp[bootstrap_indices]\n","    y_true_bootstrap = Y_test_tensor.iloc[bootstrap_indices]\n","\n","    # Compute AUROC on the bootstrap sample\n","    bootstrap_aurocs[i] = roc_auc_score(y_true_bootstrap, y_bootstrap)\n","\n","# Calculate the confidence interval\n","confidence_interval = np.percentile(bootstrap_aurocs, [2.5, 97.5])\n","\n","print(\"Original AUROC:\", original_auroc)\n","print(\"95% Confidence Interval:\", confidence_interval)"]},{"cell_type":"code","source":["# Compute AUROC ISS\n","auroc_iss = roc_auc_score(Y_test, X_ISS)\n","print(f\"AUROC on the test set: {auroc_iss}\")"],"metadata":{"id":"gfGbZwcton94"},"id":"gfGbZwcton94","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute AUROC TRISS\n","auroc_triss = roc_auc_score(Y_test, X_TRISS)\n","print(f\"AUROC on the test set: {auroc_triss}\")"],"metadata":{"id":"6smCURZ5tazz"},"id":"6smCURZ5tazz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now for paired bootstrap testing between MLISS and TRISS and MLISS and ISS\n","##first lets define a helper function\n","\n","def paired_bootstrap_auc_test(\n","    y_true,\n","    predA,\n","    predB,\n","    n_boot=1000,\n","    alpha=0.05,\n","    random_state=None\n","):\n","    \"\"\"\n","    Compare two correlated AUCs (predA vs. predB) via paired bootstrapping,\n","    also returning 95% CI for each AUC individually.\n","\n","    Parameters\n","    ----------\n","    y_true : array-like of shape (n_samples,)\n","        Ground truth binary labels (0 or 1).\n","    predA : array-like of shape (n_samples,)\n","        Scores (e.g., probabilities) from model/variable A.\n","    predB : array-like of shape (n_samples,)\n","        Scores from model/variable B.\n","    n_boot : int, default=1000\n","        Number of bootstrap iterations.\n","    alpha : float, default=0.05\n","        Significance level (for the (1 - alpha) CIs).\n","    random_state : int or None\n","        Random seed for reproducibility.\n","\n","    Returns\n","    -------\n","    results : dict\n","        Dictionary containing:\n","        - aucA, aucB: AUC on the original dataset for each method.\n","        - aucA_ci_lower, aucA_ci_upper: (1 - alpha) CI for A's AUC.\n","        - aucB_ci_lower, aucB_ci_upper: (1 - alpha) CI for B's AUC.\n","        - baseline_diff: AUC(A) - AUC(B) on the full dataset (no resampling).\n","        - mean_diff: Mean difference of AUCs across bootstrap samples.\n","        - diff_ci_lower, diff_ci_upper: (1 - alpha) CI for AUC difference.\n","        - p_value: Approx. two-sided p-value for difference in AUC.\n","    \"\"\"\n","\n","    # Convert to NumPy arrays\n","    y_true = np.asarray(y_true)\n","    predA = np.asarray(predA)\n","    predB = np.asarray(predB)\n","\n","    # Basic checks\n","    assert len(y_true) == len(predA) == len(predB), \"Arrays must all have the same length.\"\n","    n = len(y_true)\n","\n","    # Compute AUC on the full (original) dataset\n","    aucA = roc_auc_score(y_true, predA)\n","    aucB = roc_auc_score(y_true, predB)\n","    baseline_diff = aucA - aucB\n","\n","    # Random generator\n","    rng = np.random.default_rng(random_state)\n","\n","    # Arrays to store bootstrapped AUCs\n","    aucAs = np.zeros(n_boot)\n","    aucBs = np.zeros(n_boot)\n","    diffs = np.zeros(n_boot)\n","\n","    for i in range(n_boot):\n","        # 1) Sample n indices with replacement\n","        sample_idx = rng.integers(0, n, size=n)\n","\n","        # 2) Create bootstrap samples\n","        y_samp = y_true[sample_idx]\n","        A_samp = predA[sample_idx]\n","        B_samp = predB[sample_idx]\n","\n","        # 3) Compute AUC for each method\n","        aucA_samp = roc_auc_score(y_samp, A_samp)\n","        aucB_samp = roc_auc_score(y_samp, B_samp)\n","\n","        # 4) Store AUCs and difference\n","        aucAs[i] = aucA_samp\n","        aucBs[i] = aucB_samp\n","        diffs[i] = aucA_samp - aucB_samp\n","\n","    # Compute individual AUC CIs\n","    aucA_ci_lower = np.percentile(aucAs, 100 * (alpha / 2))\n","    aucA_ci_upper = np.percentile(aucAs, 100 * (1 - alpha / 2))\n","\n","    aucB_ci_lower = np.percentile(aucBs, 100 * (alpha / 2))\n","    aucB_ci_upper = np.percentile(aucBs, 100 * (1 - alpha / 2))\n","\n","    # Compute difference CI\n","    diff_ci_lower = np.percentile(diffs, 100 * (alpha / 2))\n","    diff_ci_upper = np.percentile(diffs, 100 * (1 - alpha / 2))\n","\n","    # Approx. two-sided p-value by sign test on \"diffs\"\n","    n_neg = np.sum(diffs < 0)\n","    n_pos = np.sum(diffs > 0)\n","    p_val = 2.0 * min(n_neg, n_pos) / n_boot\n","    p_val = min(p_val, 1.0)\n","\n","    coverage = (1 - alpha) * 100.0  # e.g., 95.0 if alpha=0.05\n","\n","    return {\n","        \"aucA\": aucA,\n","        \"aucB\": aucB,\n","        \"aucA_ci_lower\": aucA_ci_lower,\n","        \"aucA_ci_upper\": aucA_ci_upper,\n","        \"aucB_ci_lower\": aucB_ci_lower,\n","        \"aucB_ci_upper\": aucB_ci_upper,\n","        \"baseline_diff\": baseline_diff,\n","        \"mean_diff\": np.mean(diffs),\n","        \"diff_ci_lower\": diff_ci_lower,\n","        \"diff_ci_upper\": diff_ci_upper,\n","        \"p_value\": p_val,\n","        \"coverage\": coverage\n","    }\n"],"metadata":{"id":"t6W0mtG22aEO"},"id":"t6W0mtG22aEO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#now actually run paired bootstrapping\n","# Y_test: ground truth labels (0 or 1)\n","# ML_preds: predictions from your ML model\n","# ISS_vals, pTRISS_vals, TRISS_vals: numeric scores\n","\n","pairs = {\n","    \"ISS\": X_ISS.values,\n","    \"TRISS\": X_TRISS.values,\n","}\n","\n","for var_name, var_array in pairs.items():\n","    results = paired_bootstrap_auc_test(\n","        y_true=Y_test,\n","        predA=y_prob_gbo_mtp,\n","        predB=var_array,\n","        n_boot=2000,      # or more for higher precision\n","        alpha=(0.05/2),   #for 97.5% CI (bonferroni correction for 2 comparisons)\n","        random_state=42\n","    )\n","    coverage_str = f\"{results['coverage']:.1f}%\"\n","\n","    print(f\"--- ML Model vs. {var_name} ---\")\n","    print(f\"AUC(ML) = {results['aucA']:.3f}, {coverage_str} CI: \"\n","          f\"[{results['aucA_ci_lower']:.3f}, {results['aucA_ci_upper']:.3f}]\")\n","    print(f\"AUC({var_name}) = {results['aucB']:.3f}, {coverage_str} CI: \"\n","          f\"[{results['aucB_ci_lower']:.3f}, {results['aucB_ci_upper']:.3f}]\")\n","    print(f\"AUC diff (ML - {var_name}) = {results['baseline_diff']:.4f}, {coverage_str} CI: \"\n","          f\"[{results['diff_ci_lower']:.4f}, {results['diff_ci_upper']:.4f}]\")\n","    print(f\"p-value = {results['p_value']:.4f}\\n\")\n","\n"],"metadata":{"id":"KVKHaWrJ2d39"},"id":"KVKHaWrJ2d39","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now lets get 95% cI for PR curve for MLISS\n","\n","# Number of bootstrap iterations\n","n_iterations = 1000\n","# List to store bootstrapped AUPRC values\n","auprc_scores = []\n","\n","# Original precision-recall calculation for actual data\n","precision, recall, thresholds = precision_recall_curve(Y_test, y_prob_gbo_mtp)\n","pr_auc = auc(recall, precision)\n","\n","# Bootstrapping process to calculate 95% CI\n","for i in range(n_iterations):\n","    # Resample Y_test and y_prob_gbo_mtp with replacement\n","    Y_test_resampled, y_prob_resampled = resample(Y_test, y_prob_gbo_mtp)\n","\n","    # Calculate precision and recall for the bootstrap sample\n","    precision_resampled, recall_resampled, _ = precision_recall_curve(Y_test_resampled, y_prob_resampled)\n","\n","    # Calculate AUPRC for the resampled data\n","    pr_auc_resampled = auc(recall_resampled, precision_resampled)\n","\n","    # Store the AUPRC score\n","    auprc_scores.append(pr_auc_resampled)\n","\n","# Calculate the 95% confidence interval (2.5th and 97.5th percentiles)\n","lower_bound = np.percentile(auprc_scores, 2.5)\n","upper_bound = np.percentile(auprc_scores, 97.5)\n","\n","# Plot the original precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label=f'PR curve (AUC={pr_auc:.3f})')\n","\n","# Add labels and title\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.show()\n","\n","# Print the original AUPRC and 95% CI\n","print(f'AUPRC: {pr_auc:.3f}')\n","print(f'95% CI for AUPRC: [{lower_bound:.3f}, {upper_bound:.3f}]')"],"metadata":{"id":"HsRDSjt4TSfE"},"id":"HsRDSjt4TSfE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now lets get 95% cI for PR curve for ISS\n","\n","# Number of bootstrap iterations\n","n_iterations = 1000\n","# List to store bootstrapped AUPRC values\n","auprc_scores = []\n","\n","# Original precision-recall calculation for actual data\n","precision, recall, thresholds = precision_recall_curve(Y_test, iss_probs)\n","pr_auc = auc(recall, precision)\n","\n","# Bootstrapping process to calculate 95% CI\n","for i in range(n_iterations):\n","    # Resample Y_test and y_prob_gbo_mtp with replacement\n","    Y_test_resampled, y_prob_resampled = resample(Y_test,iss_probs)\n","\n","    # Calculate precision and recall for the bootstrap sample\n","    precision_resampled, recall_resampled, _ = precision_recall_curve(Y_test_resampled, y_prob_resampled)\n","\n","    # Calculate AUPRC for the resampled data\n","    pr_auc_resampled = auc(recall_resampled, precision_resampled)\n","\n","    # Store the AUPRC score\n","    auprc_scores.append(pr_auc_resampled)\n","\n","# Calculate the 95% confidence interval (2.5th and 97.5th percentiles)\n","lower_bound = np.percentile(auprc_scores, 2.5)\n","upper_bound = np.percentile(auprc_scores, 97.5)\n","\n","# Plot the original precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label=f'PR curve (AUC={pr_auc:.3f})')\n","\n","# Add labels and title\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.show()\n","\n","# Print the original AUPRC and 95% CI\n","print(f'AUPRC: {pr_auc:.3f}')\n","print(f'95% CI for AUPRC: [{lower_bound:.3f}, {upper_bound:.3f}]')"],"metadata":{"id":"iE0hE_TOUV8Z"},"id":"iE0hE_TOUV8Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now lets get 95% cI for PR curve for TRISS\n","\n","# Number of bootstrap iterations\n","n_iterations = 1000\n","# List to store bootstrapped AUPRC values\n","auprc_scores = []\n","\n","# Original precision-recall calculation for actual data\n","precision, recall, thresholds = precision_recall_curve(Y_test, X_TRISS.values)\n","pr_auc = auc(recall, precision)\n","\n","# Bootstrapping process to calculate 95% CI\n","for i in range(n_iterations):\n","    # Resample Y_test and y_prob_gbo_mtp with replacement\n","    Y_test_resampled, y_prob_resampled = resample(Y_test,X_TRISS.values)\n","\n","    # Calculate precision and recall for the bootstrap sample\n","    precision_resampled, recall_resampled, _ = precision_recall_curve(Y_test_resampled, y_prob_resampled)\n","\n","    # Calculate AUPRC for the resampled data\n","    pr_auc_resampled = auc(recall_resampled, precision_resampled)\n","\n","    # Store the AUPRC score\n","    auprc_scores.append(pr_auc_resampled)\n","\n","# Calculate the 95% confidence interval (2.5th and 97.5th percentiles)\n","lower_bound = np.percentile(auprc_scores, 2.5)\n","upper_bound = np.percentile(auprc_scores, 97.5)\n","\n","# Plot the original precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label=f'PR curve (AUC={pr_auc:.3f})')\n","\n","# Add labels and title\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.show()\n","\n","# Print the original AUPRC and 95% CI\n","print(f'AUPRC: {pr_auc:.3f}')\n","print(f'95% CI for AUPRC: [{lower_bound:.3f}, {upper_bound:.3f}]')"],"metadata":{"id":"FNzPDedKWMFB"},"id":"FNzPDedKWMFB","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"machine_shape":"hm","gpuType":"V6E1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}